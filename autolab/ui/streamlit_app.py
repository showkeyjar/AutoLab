import sys
import os
import pandas as pd
import json
import uuid

# è®¾ç½®é¡¹ç›®æ ¹ç›®å½•
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../.."))
sys.path.insert(0, project_root)

# ç°åœ¨å¯ä»¥å®‰å…¨å¯¼å…¥æœ¬åœ°æ¨¡å—
from autolab.core.experiment_state import ExperimentState
from autolab.core.experiment_templates import ExperimentTemplate
from autolab.core.task_flow import TaskFlow
from autolab.utils.llm_client import OllamaClient

# 1. å½“å‰è„šæœ¬ç»å¯¹è·¯å¾„
script_dir = os.path.dirname(os.path.abspath(__file__))
# 2. é¡¹ç›®æ ¹ç›®å½•ï¼ˆautolab çš„ä¸Šä¸€çº§ï¼‰
project_root = os.path.abspath(os.path.join(script_dir, "../.."))
sys.path.insert(0, project_root)

import streamlit as st
import traceback
import os

state_manager = ExperimentState()

def init_session_state():
    """åˆå§‹åŒ–æ‰€æœ‰session stateå˜é‡"""
    if 'metrics_config' not in st.session_state:
        st.session_state.metrics_config = {
            "accuracy": {"enabled": True, "threshold": 0.8, "calculation": "correct_count / total_count"},
            "time_cost": {"enabled": True, "threshold": 60.0},
            "custom": []
        }
    if 'templates_init' not in st.session_state:
        st.session_state.templates_init = False

init_session_state()

st.set_page_config(page_title="AutoLab æ™ºèƒ½ä½“å®éªŒç³»ç»Ÿ", layout="wide")
st.title("ğŸ§ª AutoLab æ™ºèƒ½ä½“å®éªŒç³»ç»Ÿ")
st.markdown("""
- è¾“å…¥å®éªŒç›®æ ‡ï¼Œç‚¹å‡»â€œè¿è¡Œå®éªŒæµâ€æŒ‰é’®ï¼Œä¸€é”®ä½“éªŒç«¯åˆ°ç«¯æ™ºèƒ½ä½“åä½œã€‚
- æ”¯æŒåˆ†æ­¥æŸ¥çœ‹æ¯ä¸ªæ™ºèƒ½ä½“çš„ç»“æ„åŒ–è¾“å‡ºä¸åŸå§‹ LLM å“åº”ã€‚
""")

with st.sidebar:
    st.header("âš™ï¸ ç³»ç»Ÿé…ç½®")
    # åˆå§‹åŒ–Ollamaå®¢æˆ·ç«¯
    ollama_client = OllamaClient()
    ollama_url = st.text_input(
        "Ollama API åœ°å€", 
        value=ollama_client.base_url, 
        help="å¦‚ http://localhost:11434"
    )
    
    # åœ¨é…ç½®åŒºå—ä¿®æ”¹æ¨¡å‹é€‰æ‹©é€»è¾‘
    ollama_model = st.selectbox(
        "é»˜è®¤LLMæ¨¡å‹", 
        options=ollama_client.available_models,
        index=0,
        help="å¦‚ llama3ã€qwen:14b ç­‰"
    )
    
    # æ·»åŠ æ¨¡å‹çŠ¶æ€æç¤º
    if not ollama_client.available_models:
        st.warning("æœªæ£€æµ‹åˆ°å¯ç”¨æ¨¡å‹ï¼Œè¯·ç¡®ä¿OllamaæœåŠ¡å·²è¿è¡Œ")
        st.stop()  # æ²¡æœ‰å¯ç”¨æ¨¡å‹æ—¶åœæ­¢æ‰§è¡Œ
    
    # æŒ‡æ ‡é…ç½®éƒ¨åˆ†
    with st.expander("âš™ï¸ å®éªŒæŒ‡æ ‡é…ç½®", expanded=False):
        # ä¸»æŒ‡æ ‡é…ç½®
        st.subheader("æ ¸å¿ƒæŒ‡æ ‡")
        cols = st.columns(2)
        with cols[0]:
            acc_enabled = st.checkbox("å¯ç”¨å‡†ç¡®ç‡", st.session_state.metrics_config["accuracy"]["enabled"])
            acc_threshold = st.number_input("å‡†ç¡®ç‡é˜ˆå€¼", min_value=0.0, max_value=1.0, 
                                          value=st.session_state.metrics_config["accuracy"]["threshold"], step=0.05)
        with cols[1]:
            time_enabled = st.checkbox("å¯ç”¨è€—æ—¶ç»Ÿè®¡", st.session_state.metrics_config["time_cost"]["enabled"])
            time_threshold = st.number_input("æœ€å¤§è€—æ—¶(ç§’)", min_value=0.0, 
                                           value=st.session_state.metrics_config["time_cost"]["threshold"])
        
        # è‡ªå®šä¹‰æŒ‡æ ‡
        st.subheader("è‡ªå®šä¹‰æŒ‡æ ‡")
        for i, custom in enumerate(st.session_state.metrics_config["custom"]):
            with st.expander(f"æŒ‡æ ‡ {i+1}: {custom['name']}", expanded=False):
                st.text_area(f"è®¡ç®—å…¬å¼", value=custom.get("formula", ""), key=f"custom_formula_{i}")
        
        if st.button("ä¿å­˜é…ç½®"):
            st.session_state.metrics_config.update({
                "accuracy": {"enabled": acc_enabled, "threshold": acc_threshold},
                "time_cost": {"enabled": time_enabled, "threshold": time_threshold}
            })
            st.success("æŒ‡æ ‡é…ç½®å·²ä¿å­˜")

def show_template_guide():
    """æ˜¾ç¤ºæ¨¡æ¿å¼•å¯¼ç•Œé¢"""
    with st.sidebar:
        st.header("ğŸ“ å®éªŒæ¨¡æ¿")
        
        template_manager = ExperimentTemplate()
        
        if not template_manager.list_templates():
            with st.expander("ğŸ†• æ–°æ‰‹æŒ‡å—", expanded=True):
                st.markdown("""
                ### ç¬¬ä¸€æ¬¡ä½¿ç”¨æ¨¡æ¿ï¼Ÿ
                1. ç‚¹å‡»ä¸‹æ–¹æŒ‰é’®åˆ›å»ºé»˜è®¤æ¨¡æ¿
                2. æˆ–é€‰æ‹©"æ–°å»ºæ¨¡æ¿"åˆ›å»ºè‡ªå®šä¹‰æ¨¡æ¿
                3. æ¨¡æ¿å¯ä¿å­˜å¸¸ç”¨å®éªŒé…ç½®
                """)
                if st.button("âœ¨ ä¸€é”®åˆ›å»ºé»˜è®¤æ¨¡æ¿"):
                    init_default_templates()
                    st.rerun()
            return
        
        # æ­£å¸¸æ¨¡æ¿é€‰æ‹©ç•Œé¢
        template_name = st.selectbox("é€‰æ‹©æ¨¡æ¿", options=template_manager.list_templates())
        template_action = st.radio("æ“ä½œ", ["ä½¿ç”¨æ¨¡æ¿", "ç¼–è¾‘æ¨¡æ¿", "æ–°å»ºæ¨¡æ¿"], horizontal=True)
        
        if template_action == "æ–°å»ºæ¨¡æ¿":
            with st.form("new_template_form"):
                new_name = st.text_input("æ¨¡æ¿åç§°")
                if st.form_submit_button("åˆ›å»º"):
                    try:
                        template_manager.save_template(new_name, {"metrics": st.session_state.metrics_config})
                        st.success(f"æ¨¡æ¿'{new_name}'åˆ›å»ºæˆåŠŸ")
                        st.rerun()
                    except Exception as e:
                        st.error(f"åˆ›å»ºå¤±è´¥: {str(e)}")
        
        elif template_action == "ç¼–è¾‘æ¨¡æ¿":
            edit_config = template_manager.load_template(template_name) or {}
            edited = st.text_area("ç¼–è¾‘é…ç½®", value=json.dumps(edit_config, indent=2), height=300)
            if st.button("ä¿å­˜æ›´æ”¹"):
                try:
                    template_manager.save_template(template_name, json.loads(edited))
                    st.success("æ¨¡æ¿æ›´æ–°æˆåŠŸ")
                except Exception as e:
                    st.error(f"æ›´æ–°å¤±è´¥: {str(e)}")

def init_default_templates():
    """åˆå§‹åŒ–é»˜è®¤æ¨¡æ¿"""
    default_templates = {
        "åŸºç¡€åˆ†ç±»å®éªŒ": {
            "metrics": {"accuracy": {"threshold": 0.85}}, 
            "steps": ["æ•°æ®åŠ è½½", "ç‰¹å¾å·¥ç¨‹", "æ¨¡å‹è®­ç»ƒ", "è¯„ä¼°"]
        },
        "æ—¶é—´åºåˆ—é¢„æµ‹": {
            "metrics": {"mae": {"threshold": 0.1}, "time_cost": {"threshold": 300}},
            "steps": ["æ•°æ®é¢„å¤„ç†", "ç‰¹å¾ç”Ÿæˆ", "æ¨¡å‹è®­ç»ƒ", "é¢„æµ‹", "å¯è§†åŒ–"]
        }
    }
    
    template_manager = ExperimentTemplate()
    for name, config in default_templates.items():
        if name not in template_manager.list_templates():
            template_manager.save_template(name, config)

show_template_guide()

# å®æ—¶ç›‘æ§åŒºå—
st.header("ğŸ“ˆ å®æ—¶ç›‘æ§")
monitor_placeholder = st.empty()

saved_experiments = state_manager.list_states()
if saved_experiments:
    selected_exp = st.selectbox("æ¢å¤å®éªŒ", options=saved_experiments)
    if st.button("åŠ è½½"):
        state = state_manager.load_state(selected_exp)
        st.session_state.update(state)
        st.rerun()

def_goal = "æå‡æ°”è±¡å¤§æ¨¡å‹çš„é¢„æµ‹å‡†ç¡®ç‡"
user_goal = st.text_input("å®éªŒç›®æ ‡", def_goal, key="goal_input")

# åœ¨è¿è¡ŒæŒ‰é’®æ—æ·»åŠ ä¸­æ–­æŒ‰é’®
col1, col2 = st.columns([3,1])
with col1:
    run_btn = st.button("è¿è¡Œå®éªŒæµ")
with col2:
    stop_btn = st.button("ğŸ›‘ å¼ºåˆ¶åœæ­¢", type="secondary")

# åœ¨å®éªŒè¿è¡Œå‰æ£€æŸ¥ä¸­æ–­çŠ¶æ€
if stop_btn:  # æ£€æŸ¥æ˜¯å¦è§¦å‘ä¸­æ–­
    if 'OllamaClient' in globals():
        ollama_client.stop()
    st.warning("æ­£åœ¨åœæ­¢å½“å‰å®éªŒ...")
    st.stop()

os.environ["PYTHONPATH"] = os.getcwd()

if 'experiment_id' not in st.session_state:
    st.session_state.experiment_id = str(uuid.uuid4())
    # å°è¯•åŠ è½½å·²æœ‰çŠ¶æ€
    saved_state = state_manager.load_state(st.session_state.experiment_id)
    if saved_state:
        st.session_state.update(saved_state)

def run_taskflow(goal):
    try:
        task_flow = TaskFlow()  # ä¸ä¼  agent_manager å‚æ•°
        
        # Initialize retry tracking
        st.session_state.setdefault('attempts', 0)
        st.session_state.setdefault('previous_actions', set())
        st.session_state.setdefault('attempt_history', [])
        st.session_state.setdefault('best_score', 0)
        
        while st.session_state.attempts < 10:
            result = task_flow.run_flow({"goal": goal})
            
            # Check metrics against thresholds
            if 'metrics' in result:
                metrics_ok = True
                for metric, config in st.session_state.metrics_config.items():
                    if config.get('enabled', False) and metric in result['metrics']:
                        if result['metrics'][metric] < config.get('threshold', 0):
                            metrics_ok = False
                            break
            else:
                metrics_ok = False
            
            if metrics_ok:
                return result, None
                
            # Check for repeated actions
            if 'previous_actions' not in st.session_state:
                st.session_state.previous_actions = set()

            current_actions = frozenset(a['action'] for a in result.get('actions', []))

            # åªæœ‰å½“æœ‰å®é™…åŠ¨ä½œæ—¶æ‰æ£€æŸ¥é‡å¤
            if current_actions and current_actions in st.session_state.previous_actions:
                st.warning("âš ï¸ æ£€æµ‹åˆ°å¯èƒ½çš„é‡å¤åŠ¨ä½œåºåˆ—")
                cols = st.columns(3)
                cols[0].write("ä¸Šæ¬¡åŠ¨ä½œåºåˆ—:")
                for a in st.session_state.previous_actions:
                    cols[1].write(f"- {a}")
                
                cols[0].write("å½“å‰åŠ¨ä½œåºåˆ—:")
                for a in current_actions:
                    cols[2].write(f"- {a}")
                
                if st.checkbox("ä»ç„¶ç»§ç»­æ‰§è¡Œ"):
                    st.session_state.previous_actions.add(current_actions)
                    return result, None
                else:
                    return None, "ç”¨æˆ·é€‰æ‹©ç»ˆæ­¢æ‰§è¡Œ"
            else:
                st.session_state.previous_actions.add(current_actions)
                return result, None
            
            st.session_state.attempts += 1
            
            # Update attempt history
            attempt_history = st.session_state.attempt_history
            attempt_history.append({
                'attempt': st.session_state.attempts,
                'score': result['metrics']['accuracy']
            })
            st.session_state.attempt_history = attempt_history
            
            # Update best score
            if result['metrics']['accuracy'] > st.session_state.best_score:
                st.session_state.best_score = result['metrics']['accuracy']
            
            # Check for manual stop
            if st.session_state.get('manual_stop', False):
                return None, "äººå·¥å¹²é¢„ç»ˆæ­¢"
            
        return None, "è¶…è¿‡æœ€å¤§å°è¯•æ¬¡æ•°(10æ¬¡)ä»æœªè¾¾æ ‡"
        
    except Exception as e:
        return None, traceback.format_exc()

def generate_diagnostic_report(result, metrics_config):
    """ç”Ÿæˆå®éªŒè¯Šæ–­æŠ¥å‘Š"""
    report = {
        'summary': 'å®éªŒæœªè¾¾åˆ°é¢„æœŸç›®æ ‡',
        'failed_metrics': [],
        'action_sequence': [],
        'suggestions': []
    }
    
    # åˆ†ææœªè¾¾æ ‡æŒ‡æ ‡
    if isinstance(metrics_config, dict) and 'metrics' in result:
        for metric, config in metrics_config.items():
            if metric != 'custom':  # è·³è¿‡customåˆ—è¡¨
                if isinstance(config, dict) and config.get('enabled', False):
                    if metric in result['metrics'] and isinstance(result['metrics'][metric], (int, float)):
                        threshold = config.get('threshold', 0)
                        if result['metrics'][metric] < threshold:
                            report['failed_metrics'].append({
                                'metric': metric,
                                'actual': result['metrics'][metric],
                                'expected': threshold,
                                'delta': threshold - result['metrics'][metric]
                            })
    
    # è®°å½•æ‰§è¡Œè¿‡ç¨‹
    if 'actions' in result and isinstance(result['actions'], list):
        report['action_sequence'] = [
            f"{i+1}. {action.get('action', 'æœªçŸ¥åŠ¨ä½œ')}" 
            for i, action in enumerate(result['actions'])
        ]
    
    # ç”Ÿæˆæ”¹è¿›å»ºè®®
    if report['failed_metrics']:
        for metric in report['failed_metrics']:
            if metric['metric'] == 'accuracy':
                report['suggestions'].append("å°è¯•å¢åŠ è®­ç»ƒæ•°æ®é‡æˆ–è°ƒæ•´æ¨¡å‹å‚æ•°")
            elif metric['metric'] == 'time_cost':
                report['suggestions'].append("ä¼˜åŒ–è®¡ç®—æ­¥éª¤æˆ–å‡å°‘æ•°æ®è§„æ¨¡")
    
    return report

def show_diagnostic_report(report):
    """æ˜¾ç¤ºè¯Šæ–­æŠ¥å‘Š"""
    with st.expander("ğŸ” å®éªŒè¯Šæ–­æŠ¥å‘Š", expanded=True):
        st.subheader("é—®é¢˜æ€»ç»“")
        st.write(report['summary'])
        
        if report['failed_metrics']:
            st.subheader("æœªè¾¾æ ‡æŒ‡æ ‡")
            for metric in report['failed_metrics']:
                st.error(
                    f"{metric['metric']}: å®é™…å€¼ {metric['actual']:.2f} "
                    f"(æœŸæœ› â‰¥ {metric['expected']:.2f}, å·®è· {metric['delta']:.2f})"
                )
        
        if report['action_sequence']:
            st.subheader("æ‰§è¡Œæ­¥éª¤")
            st.write('\n'.join(report['action_sequence']))
        
        if report['suggestions']:
            st.subheader("æ”¹è¿›å»ºè®®")
            for suggestion in report['suggestions']:
                st.info(f"ğŸ’¡ {suggestion}")

if run_btn and user_goal.strip():
    try:
        with st.spinner("æ™ºèƒ½ä½“åä½œä¸­ï¼Œè¯·ç¨å€™..."):
            result, err = run_taskflow(user_goal.strip())
            if stop_btn:  # æ£€æŸ¥æ˜¯å¦è§¦å‘ä¸­æ–­
                raise RuntimeError("å®éªŒè¢«ç”¨æˆ·ä¸­æ–­")
    except RuntimeError as e:
        st.error(f"å®éªŒä¸­æ–­: {str(e)}")
    else:
        if err:
            st.error("è¿è¡Œå‡ºé”™ï¼š\n" + err)
        elif result:
            st.success("å®éªŒæµå·²å®Œæˆï¼")
            for key, val in result.items():
                st.markdown(f"### {key} æ™ºèƒ½ä½“è¾“å‡º")
                if isinstance(val, dict):
                    # ç»“æ„åŒ–ä¸»ä¿¡æ¯
                    for sk, sv in val.items():
                        if sk.endswith("llm_raw") or sk == "llm_raw":
                            continue
                        st.write(f"**{sk}**:", sv)
                    # åŸå§‹ LLM å“åº”å•ç‹¬ç”¨ code å±•ç¤ºï¼Œé¿å…åµŒå¥— expander
                    if "llm_raw" in val:
                        st.markdown("**åŸå§‹ LLM å“åº”ï¼š**")
                        st.code(val["llm_raw"], language="text")
                else:
                    st.write(val)
            
            # Add progress visualization
            if st.session_state.get('attempts', 0) > 0:
                st.subheader("å®éªŒè¿›åº¦")
                
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("å°è¯•æ¬¡æ•°", st.session_state['attempts'])
                    st.progress(min(st.session_state['attempts'] / 10, 1.0))
                
                with col2:
                    if st.session_state.get('best_score'):
                        st.metric("æœ€ä½³å¾—åˆ†", f"{st.session_state['best_score']:.2f}")
                
                # Show attempt history
                if st.session_state.get('attempt_history'):
                    history_df = pd.DataFrame(st.session_state['attempt_history'])
                    st.line_chart(history_df.set_index('attempt')['score'])
            
            # åœ¨ç»“æœå±•ç¤ºéƒ¨åˆ†æ·»åŠ ä¼˜åŒ–ä¿¡æ¯
            if result and 'optimized_params' in result:
                st.subheader("å‚æ•°ä¼˜åŒ–ç»“æœ")
                st.json(result['optimized_params'])
                
                # æ˜¾ç¤ºä¼˜åŒ–å†å²
                history = task_flow.optimizer.history
                if history:
                    df = pd.DataFrame([{
                        'attempt': i+1, 
                        'score': score,
                        **params
                    } for i, (params, score) in enumerate(history)])
                    
                    st.line_chart(df.set_index('attempt')['score'])
            
            # ä¿å­˜çŠ¶æ€
            state_manager.save_state(
                st.session_state.experiment_id,
                {
                    'goal': user_goal,
                    'result': result,
                    'history': st.session_state.get('attempt_history', []),
                    'config': {
                        'ollama_model': ollama_model,
                        'metrics_cfg': st.session_state.metrics_config
                    }
                }
            )

            # åœ¨ç»“æœæ˜¾ç¤ºéƒ¨åˆ†
            if 'metrics' in result:
                st.subheader("ğŸ“Š æŒ‡æ ‡ç»“æœ")
                cols = st.columns(3)
                for i, (metric, value) in enumerate(result['metrics'].items()):
                    cols[i % 3].metric(
                        label=metric,
                        value=f"{value:.2f}",
                        delta=f"è¾¾æ ‡" if value >= st.session_state.metrics_config.get(metric, {}).get('threshold', 0) else "æœªè¾¾æ ‡",
                        delta_color="normal"
                    )
            
            # å¢å¼ºç»“æœå¤„ç†å¥å£®æ€§
            if result.get('actions'):
                st.subheader("æ‰§è¡ŒåŠ¨ä½œ")
                for action in result['actions']:
                    st.markdown(f"- {action['action']}")
            
            # ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š
            diagnostic_report = generate_diagnostic_report(result, st.session_state.metrics_config)
            show_diagnostic_report(diagnostic_report)

else:
    st.info("è¯·åœ¨ä¸Šæ–¹è¾“å…¥å®éªŒç›®æ ‡ï¼Œç„¶åç‚¹å‡»â€œè¿è¡Œå®éªŒæµâ€æŒ‰é’®ã€‚")
